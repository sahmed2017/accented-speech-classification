# Accented Speech Classification through Audio Features Collaboration

The recognition of accented speech still remains a dominant  problem  in  automatic  speech  recognition  (ASR)  systems. Statistical  analysis  has  shown  that  accent  is  one  of  the  key factors in speaker variability that affects the performance of ASR.  The problem of classifying accented speech is approached through audio segmentation and the collaboration of several audio features, MFCC  (Mel-frequency  cepstral  coefficients), and the average of RMS  (Root-mean-square), ZCR (Zero Crossing Rate), Chromagram, Spectral Centroid, Spectral Bandwidth, and Spectral Rolloff. The  results  show  that  the  collaboration  of  the  specified audio  features,  when  unscaled,  led  to  either  a  neutral  or negative effect on the classifiersâ€™ accuracy. Frequency-domain  features,  in  particular,  were  found  to  mitigate  the scores  of  all  the  classifiers,  except  Random  Forest  (RF). These could be recognized through graphing the training and testing loss of the ANN in that they all shared an initial high value of loss that sharply decreased and gradually declined. However, scaling the features resulted in significant increases  in  accuracy  for  Support  Vector  Machines  (SVM), k-Nearest Neighbors (kNN), and the Artificial Neural Network  (ANN).  Random  Forest  (RF)  remained  consistent throughout. 


Keywords:  accented  speech  classification;  feature  scaling; audio features collaboration; audio segmentation. 
